\documentclass{article}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{inconsolata}
\usepackage{listings}
\renewcommand{\ttdefault}{cmtt}
\lstset{
    commentstyle = \color{gray},
    extendedchars=\true,
    basicstyle=\footnotesize\ttfamily\large,
    columns=fixed,
    inputencoding=utf8x,
    language = php,
    keepspaces=true,
    keywordstyle = \bfseries,
    morekeywords={function, return, new}
}

\newcounter{question}
\setcounter{question}{0}
\begin{document}

\newcommand\Que[1]{% 
   \leavevmode\par
   \stepcounter{question}
   \noindent
   \thequestion. \Large #1\par \small answer \par \normalsize }

\newcommand \Source[1]{\small source: \href{#1}#1}
\newcommand \Img[1]{\makebox[\textwidth]{\includegraphics[width=0.6\paperwidth]{#1}}}

\section{GNU R}

\Que{What data types does R have}
\begin{itemize}
    \item vector
    \item list
    \item environment
\end{itemize}
Note: there are alsow classes in R, for example data.frame

\Que{What R libraries for dataframe processign do you know}
\begin{itemize}
    \item{dplyr}
    \item{datatable}
    \item{ggplot2}
    \item {tidyr}
\end{itemize}

\Que{Given two lists x=list(1,2) and y=list(3,4) produce list containing all values in x and y preserving their order}
\begin{lstlisting}
> append(list(x), list(y))
\end{lstlisting}

\Que{Given vectors $x=(1, 2 ,3)$ and $y=(2, 4, 6)$ calculate linear regression $$y \propto \beta x$$ and print $\beta$}
\begin{lstlisting}
> x <- c(1, 2, 3)
> y <- c(2, 4, 6)
> r <- lm( y~x )
> r$coefficients[2]
\end{lstlisting}

\Que{What is INNER JOIN operation between two data frames?}
    
Given sets of tuples
     $A=(a_l, k_l)$ 
    and 
     $B=(b_r, k_r)$  with key $k$
    JOIN operation produces set of tuples 
    $(a_j, b_j, k_j)$ so that for each $j$ there exist some indexes $l_j$ and $r_j$ such that $k_{l_j}=k_{r_j}$


\Que{How one could join two data frames in R by common column "key"?}
\begin{lstlisting}
> library(dplyr)
> d1 <- tibble(a=c(1,2),key=c("one","two"))
> d2 <- tibble(b=c(10,20),key=c("one","two"))
> inner_join(d1,d2,by="key")
A tibble: 2 x 3
      a key       b
  <dbl> <chr> <dbl>
1     1 one      10
2     2 two      20
\end{lstlisting}

\section{Python}
\Que{What data types are there in Python}
\begin{itemize}
    \item Int
    \item Float
    \item dict
    \item list
    \item object
    \item np.ndarray
\end{itemize}

\Que{What vector/DataFrame libraries are there in python}
\begin{itemize}
    \item{pandas}
    \item{numpy}
\end{itemize}

\section{General Machine learning}
\Que{What dimensity reduction methods you have heard about?}
\begin{itemize}
    \item Principal Components analysis (PCA)
    \item Linear discriminant analysis (LDA)
\end{itemize}

\Que{What is list comprehension? Calculate squares of list 1,2,3}
\begin{lstlisting}
    [x*x for x in [1,2,3]]
\end{lstlisting}

\Que{Given dataframe STOCKS of stock data with three columns SYMBOL, PRICE, VOLUME how one could filter stocks with price less than \$100}
\begin{lstlisting}
    STOCKS[STOCKS.price<100]
\end{lstlisting}

\Que{How }
\begin{lstlisting}
    STOCKS[STOCKS.price<100]
\end{lstlisting}

\Que{What is Principal Components Analysis}
TODO
\small Source: \href{https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491}https://medium.com/@jonathan\_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491

\section{Math}

\subsection{Linear algebra}

\Que{Which matrix corresponds to rotation on 90 degrees anti-clockwise}
\[
    \begin{matrix} 
        cos(\phi) & -sin(\phi) \\
        sin(\phi) & cos(\phi)
    \end{matrix}    
\]
For 90 degrees 
\[
    \begin{matrix} 
        0 & -1 \\
        1 & 0
    \end{matrix}    
\]

\Que{What is eigenvector and eigenvalue}
In essence, an eigenvector v of a linear transformation $A$
is a non-zero vector that, when T is applied to it, does not change direction. 
Applying T to the eigenvector only scales the eigenvector by the scalar value $\lambda$, 
called an eigenvalue. This condition can be written as the equation

\[ \mathbf{A}x=\lambda x\]

\subsection{Probability}

\Que{What is normal distribution? Why is it important?}
\Source{wikipedia}

\[
    \phi (x)=\frac {1}{\sqrt {2\pi }} e^{-\frac {1}{2} x^2}
\]

\Que{What is the law of big numbers and central limit theorem?}
Law of big numbers:\\
If random variables $X_i$ are independent identically distributed with mean $\mu$ and finite variance $\sigma^2$, then 
their sample mean

\[ S_n=\big({\frac{1}{n}}\sum_{i=1}^{n}X_{i}\big) \] 

coverges by probability and almost surely to their mean $\mu$ 

Central limit theorem (Levy): \\
\[ Z = \sqrt{n} (S_n-\mu) \] converges in distribution to noraml distributed with mean $0$ and variance $\sigma^2$

\subsection{Dual problem, Convex optimization}
TODO
\section{Machine learning}

\Que{What is linear regression?}

\Que{Name some clusterization (classification) algorithms}
\begin{itemize}
\item k-means
\item suport vector machine (SVM)
\end{itemize}

\Que{What is overfitting problem?}
TODO

\Que{What is crossvalidation?}
TODO

\section{Deep learning. Neural networks}
\Que{What is neuron model}
\Img{neuron_model.jpeg}
\Source{https://www.cs.utoronto.ca/~fidler/teaching/2015/slides/CSC2523/CNN-tutorial.pdf}

\Que{Give examples of activation functions}

\begin{itemize}
    \item Step-function $$f(x)=\begin{cases} 1, x>0 \\ 0,  x<0 \end{cases}$$
    \item Sigmoid $$f(x)=\frac{1}{1+e^{-x}}$$
    \item TanH $$ f(x)=\tanh(x)$$
    \item ReLU $$f(x)=\max(0,x)$$
    \item Maxout $$f(x) = \max(w_0x + b_0, w_1x + b_1)$$
\end{itemize}
\Source{https://www.cs.utoronto.ca/~fidler/teaching/2015/slides/CSC2523/CNN-tutorial.pdf}

\Que{What are the strong and weak sides of sigmoid activation function}

Strong:
\begin{itemize}
    \item Captures non-linearity in the data
    \item Differentiable, thus could be used in gradient descent and backpropagation methods for calculating weights
\end{itemize}

Weak:
\begin{itemize}
    \item Problem of vanishing gradients when training network
\end{itemize}

\Source{https://towardsdatascience.com/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90}


\Que{For image and speech recognition, what kind of neural networks are better used and why?}

\begin{itemize}
    \item CNN (Convolution Neural Networks) are used for image recognition.
    \item RNN (Recurring Neural Networks) are used for speech recognition
\end{itemize}

\Source{https://www.cs.utoronto.ca/~fidler/teaching/2015/slides/CSC2523/CNN-tutorial.pdf}


\end{document}